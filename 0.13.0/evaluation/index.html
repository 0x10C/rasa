
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Evaluating and Improving Models</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/banner.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Confidence and Fallback Intents" href="../fallback/" />
    <link rel="prev" title="Language Support" href="../languages/" />
   
    
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
  <meta itemprop="image" content="https://rasa.com/assets/img/facebook-og.png">
  <meta property="og:title" content="Evaluating and Improving Models" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://rasa.com/assets/img/facebook-og.png" />
  <meta property="og:url" content="https://rasa.com/docs/nlu/master/evaluation" />
  
    <meta name="description" content="Evaluating ML models trained with Rasa NLU" />
    <meta itemprop="description" content="Evaluating ML models trained with Rasa NLU">
    <meta name="twitter:description" content="Evaluating ML models trained with Rasa NLU" />
    <meta property="og:description" content="Evaluating ML models trained with Rasa NLU" />
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@Rasa_HQ">
  <meta name="twitter:title" content="Evaluating and Improving Models">
  <meta name="twitter:creator" content="@Rasa_HQ">
  <meta name="twitter:image" content="https://rasa.com/assets/img/facebook-og.png">
    
  <link rel="stylesheet" href="../_static/xq-light.css" type="text/css" />
  <link rel="stylesheet" href="../_static/fontawesome/css/fontawesome-all.css" type="text/css" />
  <script defer type="text/javascript" src="../_static/config.js"></script>
  <script defer type="text/javascript" src="../_static/klaro.js"></script>
  <script type="text/javascript" src="https://storage.googleapis.com/docs-theme/clipboard.min.js"></script>
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" /> 
  

  </head><body>

<div class="nav-top">
  <a href="/docs" class="brand-link">
      <h1 class="brand">
          <img src="../_static/rasa_logo.svg" width="80px" height="40px" title="Rasa" alt="Rasa logo">
      </h1>
      <span class="logo extension">docs</span>
  </a>
  
  <ul class="nav">
    <input type="text" class="search" placeholder="Search">
    
      
        <li><a href=/docs/getting-started>Getting Started</a></li>
      
    
      
        <li class="active"><a href=/docs/nlu>NLU</a></li>
      
    
      
        <li><a href=/docs/core>Core</a></li>
      
    
      
        <li><a href=/docs/platform>Platform</a></li>
      
    
    
    <li>
      <a href="https://forum.rasa.com" target="_blank"><button class="button btn-ghost btn-small">Join Community</button></a>
    </li>
    
  </ul>
</div>

  <div class="sidebar-extended"></div>
  <div class="document">
    
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/">Try It Out</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../choosing_pipeline/">Choosing a Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../entities/">Entity Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../languages/">Language Support</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluating and Improving Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fallback/">Confidence and Fallback Intents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/">Frequently Asked Questions</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataformat/">Training Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline/">Pipeline and Component Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config/">Server Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../http/">HTTP API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../persist/">Storing Models in the Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker/">Running in Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../customcomponents/">Custom Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migrations/">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license/">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog/">Change Log</a></li>
</ul>

<div class="versions">
    <div>
      <span class="current-version">
        v: 0.13.0
      </span>
    </div>
    <div class="other-versions">
        <div class="dropdown">
          <button class="dropdown-btn">tags</button>
          <div class="dropdown-content">
              <a href="evaluation.html">0.13.0</a>
              <a href="../0.12.3/evaluation.html">0.12.3</a>
              <a href="../0.12.2/evaluation.html">0.12.2</a>
              <a href="../0.12.1/evaluation.html">0.12.1</a>
              <a href="../0.12.0/evaluation.html">0.12.0</a>
              <a href="../0.11.4/evaluation.html">0.11.4</a>
              <a href="../0.10.6/index.html">0.10.6</a>
              <a href="../0.9.2/index.html">0.9.2</a>
              <a href="../0.8.12/index.html">0.8.12</a>
              <a href="../0.7.4/index.html">0.7.4</a>
          </div>
        </div>
        <div class="dropdown">
          <button class="dropdown-btn">tags</button>
          <div class="dropdown-content">
              <a href="../master/evaluation.html">master</a>
          </div>
        </div>
    </div>
</div>


        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  





<div class="section" id="evaluating-and-improving-models">
<span id="section-evaluation"></span><h1>Evaluating and Improving Models<a class="headerlink" href="#evaluating-and-improving-models" title="Permalink to this headline">¶</a></h1>
<div class="section" id="improving-your-models-from-feedback">
<h2>Improving your models from feedback<a class="headerlink" href="#improving-your-models-from-feedback" title="Permalink to this headline">¶</a></h2>
<p>Once you have a version of your bot running, the Rasa NLU server will log
every request made to the <code class="docutils literal notranslate"><span class="pre">/parse</span></code> endpoint to a file. By default
these are saved in the folder <code class="docutils literal notranslate"><span class="pre">logs</span></code>.</p>
<div class="highlight-javascript notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;user_input&quot;</span><span class="o">:</span><span class="p">{</span>
    <span class="s2">&quot;entities&quot;</span><span class="o">:</span><span class="p">[]</span>   <span class="p">],</span>
    <span class="s2">&quot;intent&quot;</span><span class="o">:</span><span class="p">{</span>
      <span class="s2">&quot;confidence&quot;</span><span class="o">:</span><span class="mf">0.32584617693743012</span><span class="p">,</span>
      <span class="s2">&quot;name&quot;</span><span class="o">:</span><span class="s2">&quot;restaurant_search&quot;</span>
    <span class="p">},</span>
    <span class="s2">&quot;text&quot;</span><span class="o">:</span><span class="s2">&quot;nice thai places&quot;</span><span class="p">,</span>
    <span class="s2">&quot;intent_ranking&quot;</span><span class="o">:</span><span class="p">[</span> <span class="p">...</span> <span class="p">]</span>
  <span class="p">},</span>
  <span class="p">...</span>
  <span class="s2">&quot;model&quot;</span><span class="o">:</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
  <span class="s2">&quot;log_time&quot;</span><span class="o">:</span><span class="mf">1504092543.036279</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The things your users say are the best source of training data for refining your models.
Of course your model won’t be perfect, so you will have to manually go through
each of these predictions and correct any mistakes before adding them to your training data.
In this case, the entity ‘thai’ was not picked up as a cuisine.</p>
</div>
<div class="section" id="evaluating-models">
<h2>Evaluating Models<a class="headerlink" href="#evaluating-models" title="Permalink to this headline">¶</a></h2>
<p>How is your model performing? Do you have enough data? Are your intents and entities well-designed?</p>
<p>Rasa NLU has an <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> mode which helps you answer these questions.
A standard technique in machine learning is to keep some data separate as a <em>test set</em>.
If you’ve done this, you can see how well your model predicts the test cases using this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m rasa_nlu.evaluate <span class="se">\</span>
    --data data/examples/rasa/demo-rasa.json <span class="se">\</span>
    --model projects/default/model_20180323-145833
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">--data</span></code> argument points to your test data, and <code class="docutils literal notranslate"><span class="pre">--model</span></code> points to your trained model.</p>
<p>If you don’t have a separate test set, you can
still estimate how well your model generalises using cross-validation.
To do this, run the evaluation script with the <code class="docutils literal notranslate"><span class="pre">--mode</span> <span class="pre">crossvalidation</span></code> flag.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m rasa_nlu.evaluate <span class="se">\</span>
    --data data/examples/rasa/demo-rasa.json <span class="se">\</span>
    --config sample_configs/config_spacy.yml <span class="se">\</span>
    --mode crossvalidation
</pre></div>
</div>
<p>You cannot specify a model in this mode because
a new model will be trained on part of the data
for every cross-validation fold.</p>
<div class="section" id="example-output">
<h3>Example Output<a class="headerlink" href="#example-output" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="intent-classification">
<h2>Intent Classification<a class="headerlink" href="#intent-classification" title="Permalink to this headline">¶</a></h2>
<p>The evaluation script will log precision, recall, and f1 measure for
each intent and once summarized for all.
Furthermore, it creates a confusion matrix for you to see which
intents are mistaken for which others.
Samples which have not been predicted correctly are logged and saved to a file
called <code class="docutils literal notranslate"><span class="pre">errors.json</span></code> for easier debugging.
Finally, the evaluation script creates a histogram of the confidence distribution for all predictions.
Improving the quality of your training data will move the histogram bars to the right.</p>
</div>
<div class="section" id="entity-extraction">
<h2>Entity Extraction<a class="headerlink" href="#entity-extraction" title="Permalink to this headline">¶</a></h2>
<p>For each entity extractor, the evaluation script
logs its performance per entity type in your training data.
So if you use <code class="docutils literal notranslate"><span class="pre">ner_crf</span></code> and <code class="docutils literal notranslate"><span class="pre">ner_duckling_http</span></code>
in your pipeline, it will log two evaluation tables
containing recall, precision, and f1 measure for each entity type.</p>
<p>In the case <code class="docutils literal notranslate"><span class="pre">ner_duckling_http</span></code> we actually run the evaluation for
each defined duckling dimension. If you use the <code class="docutils literal notranslate"><span class="pre">time</span></code> and <code class="docutils literal notranslate"><span class="pre">ordinal</span></code>
dimensions, you would get two evaluation tables: one for
<code class="docutils literal notranslate"><span class="pre">ner_duckling_http</span> <span class="pre">(Time)</span></code> and one for <code class="docutils literal notranslate"><span class="pre">ner_duckling_http</span> <span class="pre">(Ordinal)</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">ner_synonyms</span></code> does not create an evaluation table, because it only changes the value of the found
entities and does not find entity boundaries itself.</p>
<p>Finally, keep in mind that entity types in your testing data have to match the output
of the extraction components. This is particularly important for
<code class="docutils literal notranslate"><span class="pre">ner_duckling_http</span></code>, because it is not fit to your training data.</p>
<div class="section" id="entity-scoring">
<h3>Entity Scoring<a class="headerlink" href="#entity-scoring" title="Permalink to this headline">¶</a></h3>
<p>To evaluate entity extraction we apply a simple tag-based approach. We don’t consider BILOU tags, but only the
entity type tags on a per token basis. For location entity like “near Alexanderplatz” we
expect the labels “LOC” “LOC” instead of the BILOU-based “B-LOC” “L-LOC”. Our approach is more lenient
when it comes to evaluation, as it rewards partial extraction and does not punish the splitting of entities.
For example, the given the aforementioned entity “near Alexanderplatz” and a system that extracts
“Alexanderplatz”, this reward the extraction of “Alexanderplatz” and punish the missed out word “near”.
The BILOU-based approach, however, would label this as a complete failure since it expects Alexanderplatz
to be labeled as a last token in an entity (L-LOC) instead of a single token entity (U-LOC). Also note,
a splitted extraction of “near” and “Alexanderplatz” would get full scores on our approach and zero on the
BILOU-based one.</p>
<p>Here’s a comparison between the two scoring mechanisms for the phrase “near Alexanderplatz tonight”:</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="24%" />
<col width="27%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">extracted</th>
<th class="head">Simple tags (score)</th>
<th class="head">BILOU tags (score)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>[near Alexanderplatz](loc) [tonight](time)</td>
<td>loc loc time (3)</td>
<td>B-loc L-loc U-time (3)</td>
</tr>
<tr class="row-odd"><td>[near](loc) [Alexanderplatz](loc) [tonight](time)</td>
<td>loc loc time (3)</td>
<td>U-loc U-loc U-time (1)</td>
</tr>
<tr class="row-even"><td>near [Alexanderplatz](loc) [tonight](time)</td>
<td>O   loc time (2)</td>
<td>O     U-loc U-time (1)</td>
</tr>
<tr class="row-odd"><td>[near](loc) Alexanderplatz [tonight](time)</td>
<td>loc O   time (2)</td>
<td>U-loc O     U-time (1)</td>
</tr>
<tr class="row-even"><td>[near Alexanderplatz tonight](loc)</td>
<td>loc loc loc  (2)</td>
<td>B-loc I-loc L-loc  (1)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="evaluation-parameters">
<h2>Evaluation Parameters<a class="headerlink" href="#evaluation-parameters" title="Permalink to this headline">¶</a></h2>
<p>There are a number of parameters you can pass to the evaluation script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python -m rasa_nlu.evaluate --help
</pre></div>
</div>
<p>Here is a quick overview:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>usage: evaluate.py [-h] -d DATA [--mode MODE] [-c CONFIG] [-m MODEL]
                   [-f FOLDS] [--errors ERRORS] [--histogram HISTOGRAM]
                   [--confmat CONFMAT] [--debug] [-v]

evaluate a Rasa NLU pipeline with cross validation or on external data

optional arguments:
  -h, --help            show this help message and exit
  -d DATA, --data DATA  file containing training/evaluation data
  --mode MODE           evaluation|crossvalidation (evaluate pretrained model
                        or train model by crossvalidation)
  -c CONFIG, --config CONFIG
                        model configurion file (crossvalidation only)
  -m MODEL, --model MODEL
                        path to model (evaluation only)
  -f FOLDS, --folds FOLDS
                        number of CV folds (crossvalidation only)
  --errors ERRORS       output path for the json with wrong predictions
  --histogram HISTOGRAM
                        output path for the confidence histogram
  --confmat CONFMAT     output path for the confusion matrix plot
  --debug               Print lots of debugging statements. Sets logging level
                        to DEBUG
  -v, --verbose         Be verbose. Sets logging level to INFO
</pre></div>
</div>
</div>
</div>


	    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script> 
	    <script type="text/javascript"> docsearch({ 
	     apiKey: '1f9e0efb89e98543f6613a60f847b176', 
	     indexName: 'rasa', 
	     inputSelector: 'body > div.nav-top > .nav > input', 
	     debug: false // Set debug to true if you want to inspect the dropdown 
	    }); 
	    </script> 
          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2018, Rasa Technologies GmbH | <a href="https://rasa.com/imprint/" target="_blank">Imprint</a> | <a href="https://rasa.com/privacy-policy/" target="_blank">Privacy Policy</a> 
      

    </div>

    

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async type="opt-in" data-name="googleanalytics" data-type="text/javascript" data-src="https://www.googletagmanager.com/gtag/js?id=UA-87333416-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-87333416-1', {
      'anonymize_ip': true,
    });
  </script>
  <script type="text/javascript">
    var clipboard = new ClipboardJS('.copyable');
    clipboard.on('success', function(e) {
      gtag('event', e.action, {
        'event_category': 'code',
        'event_label': e.text
      });
      const id = e.text.slice(0,3);
      document.getElementById(id).classList.add('visible');
      setTimeout(function(){ 
        document.getElementById(id).classList.remove('visible');}, 
        800
      );
    });
    clipboard.on('error', function(e) {
      console.log(e);
    });
  </script>
  </body>
</html>